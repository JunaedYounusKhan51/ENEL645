{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(ENEL645) BiLSTM_Doc_Smell.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Installing Necessary Libs"],"metadata":{"id":"OVCXm3vYsarb"}},{"cell_type":"code","metadata":{"id":"jGuiOxBdFh3k"},"source":["!pip install -q keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"utYxkMR9F0wq"},"source":["!pip install -q pydrive"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Google Drive Access"],"metadata":{"id":"che2tixLsS5T"}},{"cell_type":"code","metadata":{"id":"MpxvQ14HFYM1","executionInfo":{"status":"ok","timestamp":1603372730922,"user_tz":-360,"elapsed":23776,"user":{"displayName":"Md. Tawkat Islam Khondaker","photoUrl":"","userId":"12040636431562290492"}},"outputId":"11305bfe-0d0b-45ed-b975-f915a9103f12","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OIJwvszjF4AZ"},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Specifying Project Path"],"metadata":{"id":"2mI7HEpnsd3q"}},{"cell_type":"code","metadata":{"id":"HRjYe1E_DNdu"},"source":["# Specifying the project path (where all the corresponding files exist)\n","project_path = '/content/drive/My Drive/Code Documentation Project/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Importing necessary libs"],"metadata":{"id":"NvtxaDwMsmbj"}},{"cell_type":"code","source":["import re\n","import string\n","import numpy as np\n","import pandas as pd\n","from nltk.corpus import stopwords\n","from nltk import re, SnowballStemmer\n","from keras.models import Sequential,load_model\n","from keras.layers import Dense,Dropout\n","from keras.layers import LSTM,Conv1D, Bidirectional\n","from keras.layers import MaxPooling1D\n","from keras.layers import Flatten\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","from keras import optimizers\n","from keras.layers import TimeDistributed\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import pickle\n","\n","from sklearn.preprocessing import LabelEncoder\n","from keras import callbacks\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","import pandas as pd\n","import numpy as np\n","import pickle\n","\n","from sklearn.metrics import classification_report,precision_recall_fscore_support\n","from sklearn.metrics import precision_score,recall_score,f1_score\n","from sklearn.metrics import accuracy_score,jaccard_similarity_score, hamming_loss\n","\n","from sklearn.preprocessing import LabelEncoder\n","import glob\n","\n","import os\n","\n","import gc\n","import keras.backend as K\n","import numpy as np"],"metadata":{"id":"NNGV95IssQx7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Defining Some Necessary Functions"],"metadata":{"id":"um_bGs9vsjsS"}},{"cell_type":"code","metadata":{"id":"18swS5-OzmZA"},"source":["\n","\n","def clean_text(text):\n","    import nltk\n","    nltk.download('stopwords')\n","    ## Remove puncuation\n","    #text = text.translate(string.punctuation)\n","    translate_table = dict((ord(char), None) for char in string.punctuation)\n","    text = text.translate(translate_table)\n","\n","\n","\n","    text = re.sub(r\"\\n\", \" \", text)\n","    text = re.sub(r\"\\r\", \" \", text)\n","\n","    ## Convert words to lower case and split them\n","    text = text.lower().split()\n","\n","    ## Remove stop words\n","    stops = set(stopwords.words(\"english\"))\n","    text = [w for w in text if not w in stops and len(w) >= 3]\n","\n","    text = \" \".join(text)\n","    ## Clean the text\n","    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\",\", \" \", text)\n","    text = re.sub(r\"\\.\", \" \", text)\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\/\", \" \", text)\n","    text = re.sub(r\"\\^\", \" ^ \", text)\n","    text = re.sub(r\"\\+\", \" + \", text)\n","    text = re.sub(r\"\\-\", \" - \", text)\n","    text = re.sub(r\"\\=\", \" = \", text)\n","    text = re.sub(r\"'\", \" \", text)\n","    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n","    text = re.sub(r\":\", \" : \", text)\n","    text = re.sub(r\" e g \", \" eg \", text)\n","    text = re.sub(r\" b g \", \" bg \", text)\n","    text = re.sub(r\" u s \", \" american \", text)\n","    text = re.sub(r\"\\0s\", \"0\", text)\n","    text = re.sub(r\" 9 11 \", \"911\", text)\n","    text = re.sub(r\"e - mail\", \"email\", text)\n","    text = re.sub(r\"j k\", \"jk\", text)\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\n","    ## Stemming\n","    text = text.split()\n","    stemmer = SnowballStemmer('english')\n","    stemmed_words = [stemmer.stem(word) for word in text]\n","    text = \" \".join(stemmed_words)\n","\n","\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Defining Model Architecture"],"metadata":{"id":"90RipNPFsqgE"}},{"cell_type":"code","metadata":{"id":"AD8erKnQz53P"},"source":["def create_model_BiLSTM(vocabulary_size=400000,embedding_size=100,embedding_matrix,num_class=5):\n","    ## create model\n","    model = Sequential()\n","    model.add(Embedding(vocabulary_size, embedding_size, weights=[embedding_matrix],trainable=False))\n","\n","    model.add(Bidirectional(LSTM(300)))\n","\n","\n","    model.add(Dense(num_class, activation='sigmoid'))\n","\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    model.summary()\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading Dataset"],"metadata":{"id":"l16EX2ZNsyKy"}},{"cell_type":"code","metadata":{"id":"dOkR7D991s_c"},"source":["# Loading old dataset\n","prev_df = pd.read_excel(project_path + 'Documentation Smell (Extension)/Dataset/labelled_dataset_full_SANER.xlsx')\n","\n","\n","# Loading new dataset\n","# reading the file names of new df\n","all_files = glob.glob(project_path+\"Documentation Smell (Extension)/Dataset/all labelled sample sets-extension/\" + \"/*.xlsx\")\n","\n","new_df = pd.DataFrame()\n","\n","# loading new file contents in the new_df\n","for filename in all_files:\n","    df = pd.read_excel(filename)\n","    try:\n","      df['Documentation Text'] = df['Documentation Text'].apply(clean_text)\n","    except:\n","      print(filename)\n","      pass\n","    new_df = new_df.append(df,ignore_index=True)\n","\n","# clearing memory\n","del all_files\n","del df\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Set (Old dataset)"],"metadata":{"id":"nTjJ8mI9s2Xb"}},{"cell_type":"code","metadata":{"id":"kYGBkoLT0bAP"},"source":["texts_train=prev_df['Documentation Text']\n","texts_train=texts_train.map(lambda x: clean_text(x))\n","\n","y_train=prev_df.iloc[:,1:6].values\n","\n","vocabulary_size = 400000\n","#***********\n","time_step=300\n","embedding_size=100\n","\n","\n","\n","tokenizer_train=Tokenizer(num_words=vocabulary_size)\n","tokenizer_train.fit_on_texts(texts_train)\n","encoded_train=tokenizer_train.texts_to_sequences(texts=texts_train)\n","#print(encoded_docs)\n","vocab_size_train = len(tokenizer_train.word_index) + 1\n","print(vocab_size_train)\n","\n","X_train = sequence.pad_sequences(encoded_train, maxlen=time_step,padding='post')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Glove Embedding"],"metadata":{"id":"GxUNDZGVs6zC"}},{"cell_type":"code","metadata":{"id":"bIJ02Rjg3-q5"},"source":["f = open(project_path + 'glove.6B.100d.txt',encoding='utf-8')\n","embeddings_train={}\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_train[word] = coefs\n","f.close()\n","\n","print('Total %s word vectors.' % len(embeddings_train))\n","\n","# create a weight matrix for words in training docs\n","embedding_matrix = np.zeros((vocab_size_train, embedding_size))\n","for word, i in tokenizer_train.word_index.items():\n","\tembedding_vector_train = embeddings_train.get(word)\n","\tif embedding_vector_train is not None:\n","\t\tembedding_matrix[i] = embedding_vector_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Model"],"metadata":{"id":"dongC4Dfs_Yr"}},{"cell_type":"code","metadata":{"id":"UdCSH1b1e86c"},"source":["vocabulary_size=embedding_matrix.shape[0]\n","model=create_model_BiLSTM(vocabulary_size,embedding_size,embedding_matrix, num_class=5)\n","history=model.fit(X_train,y_train,batch_size=256,epochs=10,validation_split=0.1,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test set (new dataset)"],"metadata":{"id":"WroHGGCBtBhS"}},{"cell_type":"code","source":["texts_test=new_df['Documentation Text']\n","texts_test=texts_test.map(lambda x: clean_text(x))\n","\n","y_test=new_df.iloc[:,1:6].values\n","\n","\n","X_test = sequence.pad_sequences(encoded_train, maxlen=time_step,padding='post')"],"metadata":{"id":"jAwN2f9cqvcb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prediction with model"],"metadata":{"id":"lxChXbwwtGIC"}},{"cell_type":"code","source":["pred=model.predict(X_test)\n","\n","\n","pred_binary=np.array(pred)\n","for i in range(len(pred_binary)):\n","  for j in range(len(pred_binary[i])):\n","    pred_binary[i][j]=int(1*(pred_binary[i][j]>0.5))"],"metadata":{"id":"5aWLjWeJtH5q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Result Generation"],"metadata":{"id":"7kcs0j4NtIvK"}},{"cell_type":"code","metadata":{"id":"yM10gYjBfq8P"},"source":["y_true_np = np.array(y_test)\n","y_pred_np = np.array(pred_binary)\n","\n","num_classes = 5\n","for i in range(num_classes):\n","  print('Class: ' + str(i))\n","  new_y_true = y_true_np[:,i]\n","  new_y_pred = y_pred_np[:,i]\n","\n","  print('Accuracy for class ' + str(i) + ': ' + str(accuracy_score(new_y_true,new_y_pred)))\n","  print('Classification Report for class' + str(i) + ': ' + str(classification_report(new_y_true,new_y_pred)))"],"execution_count":null,"outputs":[]}]}